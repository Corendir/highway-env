{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from keyboard import read_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\"\n",
    "else:  \n",
    "  dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"highway-fast-v0\")\n",
    "env.config[\"duration\"] = 100\n",
    "observation_dimension = env.observation_space.shape[0]\n",
    "# print(\"observation_dimension: \", observation_dimension)\n",
    "n_acts = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "####### BUILDING A NEURAL NETWORK ###########\n",
    "##### REPRESENTING A STOCHASTIC POLICY ######\n",
    "#############################################\n",
    "\n",
    "# net_stochastic_policy is a neural network representing a stochastic policy:\n",
    "# it takes as inputs observations and outputs logits for each action\n",
    "net_stochastic_policy = nn.Sequential(\n",
    "    # nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=36, out_features=36),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=36, out_features=n_acts)\n",
    "    )\n",
    "net_stochastic_policy.to(dev)\n",
    "old_net_stochastic_policy = nn.Sequential(\n",
    "    # nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=36, out_features=36),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=36, out_features=n_acts)\n",
    "    )\n",
    "old_net_stochastic_policy.to(dev)\n",
    "\n",
    "#### DECOMMENT THE FOLLOWING LINE TO LOAD A PRE-TRAINED MODEL ###\n",
    "net_stochastic_policy.load_state_dict(torch.load(\"models PPO/model_2\"))\n",
    "#################################################################\n",
    "\n",
    "# policy inputs an observation and computes a distribution on actions\n",
    "def policy(observation):\n",
    "    if len(observation.shape) == 2:\n",
    "        observation = observation.unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        observation = observation.unsqueeze(1)\n",
    "    # print(\"\\ndimension en entrée du réseau : \", observation.shape)\n",
    "    logits = net_stochastic_policy(observation).squeeze(1)\n",
    "    # print(\"\\ndimension des logits : \", logits.shape)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "def old_policy(observation):\n",
    "    if len(observation.shape) == 2:\n",
    "        observation = observation.unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        observation = observation.unsqueeze(1)\n",
    "    # print(\"\\ndimension en entrée du réseau : \", observation.shape)\n",
    "    logits = old_net_stochastic_policy(observation).squeeze(1)\n",
    "    # print(\"\\ndimension des logits : \", logits.shape)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "# choose an action (outputs an int sampled from policy)\n",
    "def choose_action(observation):\n",
    "    # print(\"\\ndimension en entrée de choose_action : \", observation.shape)\n",
    "    observation = torch.as_tensor(observation, dtype=torch.float32).to(dev)\n",
    "    return policy(observation).sample().item()\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(batch_observations, batch_actions, batch_weights):\n",
    "    batch_logprobability = policy(batch_observations).log_prob(batch_actions)\n",
    "    batch_old_logprobability = old_policy(batch_observations).log_prob(batch_actions)\n",
    "    # clip ratio to avoid instability of POO\n",
    "    ratio = (batch_logprobability/batch_old_logprobability).clip(1-learning_rate,1+learning_rate)\n",
    "    # return -(batch_logprobability * batch_weights).mean()\n",
    "    return -(batch_logprobability * ratio * batch_weights).mean()\n",
    "\n",
    "### Constants for training\n",
    "learning_rate = 1e-2\n",
    "epochs = 10 # 50\n",
    "batch_size = 2000 # 5000\n",
    "##########################\n",
    "\n",
    "# make optimizer\n",
    "optimizer = Adam(net_stochastic_policy.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "######### VANILLA POLICY GRADIENT ###########\n",
    "#############################################\n",
    "\n",
    "def vanilla_policy_gradient():\n",
    "    for i in range(epochs):\n",
    "        batch_observations = [] \n",
    "        batch_actions = []      \n",
    "        batch_weights = []\n",
    "        batch_weights_old = []\n",
    "        batch_returns = []      \n",
    "        batch_lengths = []      \n",
    "\n",
    "        observation = env.reset()\n",
    "        # print(\"observation actual dimension: \", observation.shape)\n",
    "        # print(\"observation: \", observation)\n",
    "        done = False            \n",
    "        rewards_in_episode = []            # list for rewards in the current episode\n",
    "\n",
    "        # First step: collect experience by simulating the environment with current policy\n",
    "        while True:\n",
    "            # print(\"vroum\")\n",
    "            batch_observations.append(observation.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            action = choose_action(observation)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_actions.append(action)\n",
    "            rewards_in_episode.append(reward)\n",
    "\n",
    "            if done:\n",
    "                # print(\"\\ncrash\")\n",
    "                # if episode is over, record info about episode\n",
    "                episode_return, episode_length = sum(rewards_in_episode), len(rewards_in_episode)\n",
    "                batch_returns.append(episode_return)\n",
    "                batch_lengths.append(episode_length)\n",
    "\n",
    "                # the weight for each logprobability(action|observation)\n",
    "                batch_weights += [episode_return] * episode_length\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                observation, done, rewards_in_episode = env.reset(), False, []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_observations) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # Step second: update the policy\n",
    "        # if i==0:\n",
    "        #     batch_weights_old = batch_weights.copy()\n",
    "        # ratio = np.divide(batch_weights, batch_weights_old)\n",
    "        # if ratio.all() > 1+learning_rate:\n",
    "        #     ratio = 1+learning_rate\n",
    "        # if ratio.all() < 1-learning_rate:\n",
    "        #     ratio = 1-learning_rate\n",
    "        # J = batch_weights*ratio\n",
    "        # take a single policy gradient update step\n",
    "        old_net_dict = copy.deepcopy(net_stochastic_policy.state_dict())\n",
    "        if(i == 0):\n",
    "            old_net_stochastic_policy.load_state_dict(old_net_dict)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(torch.as_tensor(batch_observations, dtype=torch.float32).to(dev),\n",
    "                                  torch.as_tensor(batch_actions, dtype=torch.int32).to(dev),\n",
    "                                  torch.as_tensor(batch_weights, dtype=torch.float32).to(dev)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if i>0:\n",
    "            old_net_stochastic_policy.load_state_dict(old_net_dict)\n",
    "\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t episode_length: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_returns), np.mean(batch_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 19.411 \t return: 8.217 \t episode_length: 10.822\n",
      "epoch:   1 \t loss: 21.024 \t return: 9.126 \t episode_length: 12.164\n",
      "epoch:   2 \t loss: 20.731 \t return: 8.929 \t episode_length: 12.006\n",
      "epoch:   3 \t loss: 23.982 \t return: 11.004 \t episode_length: 14.765\n",
      "epoch:   4 \t loss: 22.899 \t return: 10.536 \t episode_length: 14.355\n",
      "epoch:   5 \t loss: 24.091 \t return: 11.841 \t episode_length: 16.104\n",
      "epoch:   6 \t loss: 25.094 \t return: 13.368 \t episode_length: 18.400\n",
      "epoch:   7 \t loss: 25.677 \t return: 14.662 \t episode_length: 20.253\n",
      "epoch:   8 \t loss: 25.709 \t return: 16.005 \t episode_length: 22.333\n",
      "epoch:   9 \t loss: 25.105 \t return: 16.864 \t episode_length: 23.512\n"
     ]
    }
   ],
   "source": [
    "vanilla_policy_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DECOMMENT TO SAVE THE MODEL, BE CAREFUL OF ERASING ANOTHER ONE ###\n",
    "# torch.save(net_stochastic_policy.state_dict(), \"models PPO/model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Average score of the policy:  9.536407168804105\n"
     ]
    }
   ],
   "source": [
    "###### EVALUATION ############\n",
    "\n",
    "def run_episode(env, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = choose_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    print(\"episode done\")\n",
    "    return total_reward\n",
    "\n",
    "policy_scores = [run_episode(env) for _ in range(20)] #100\n",
    "print(\"Average score of the policy: \", np.mean(policy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Press r to restart simulation, otherwise another key\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Press r to restart simulation, otherwise another key\n"
     ]
    }
   ],
   "source": [
    "###### DEMONSTRATION ############\n",
    "\n",
    "go = True\n",
    "while go:\n",
    "    for _ in range(5):\n",
    "        run_episode(env, True)\n",
    "    print(\"Press r to restart simulation, otherwise another key\")\n",
    "    if read_key() != \"r\":\n",
    "        go = False\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
