{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from keyboard import read_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\"\n",
    "else:  \n",
    "  dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"highway-fast-v0\")\n",
    "env.config[\"duration\"] = 100\n",
    "observation_dimension = env.observation_space.shape[0]\n",
    "# print(\"observation_dimension: \", observation_dimension)\n",
    "n_acts = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "####### BUILDING A NEURAL NETWORK ###########\n",
    "##### REPRESENTING A STOCHASTIC POLICY ######\n",
    "#############################################\n",
    "\n",
    "# net_stochastic_policy is a neural network representing a stochastic policy:\n",
    "# it takes as inputs observations and outputs logits for each action\n",
    "net_stochastic_policy = nn.Sequential(\n",
    "    # nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=36, out_features=36),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=36, out_features=n_acts)\n",
    "    )\n",
    "net_stochastic_policy.to(dev)\n",
    "\n",
    "#### DECOMMENT THE FOLLOWING LINE TO LOAD A PRE-TRAINED MODEL ###\n",
    "net_stochastic_policy.load_state_dict(torch.load(\"models/model_2\"))\n",
    "#################################################################\n",
    "\n",
    "# policy inputs an observation and computes a distribution on actions\n",
    "def policy(observation):\n",
    "    if len(observation.shape) == 2:\n",
    "        observation = observation.unsqueeze(0).unsqueeze(0)\n",
    "    else:\n",
    "        observation = observation.unsqueeze(1)\n",
    "    # print(\"\\ndimension en entrée du réseau : \", observation.shape)\n",
    "    logits = net_stochastic_policy(observation).squeeze(1)\n",
    "    # print(\"\\ndimension des logits : \", logits.shape)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "# choose an action (outputs an int sampled from policy)\n",
    "def choose_action(observation):\n",
    "    # print(\"\\ndimension en entrée de choose_action : \", observation.shape)\n",
    "    observation = torch.as_tensor(observation, dtype=torch.float32).to(dev)\n",
    "    return policy(observation).sample().item()\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(batch_observations, batch_actions, batch_weights):\n",
    "    batch_logprobability = policy(batch_observations).log_prob(batch_actions)\n",
    "    return -(batch_logprobability * batch_weights).mean()\n",
    "\n",
    "### Constants for training\n",
    "learning_rate = 1e-2\n",
    "epochs = 200 # 50\n",
    "batch_size = 1000 # 5000\n",
    "##########################\n",
    "\n",
    "# make optimizer\n",
    "optimizer = Adam(net_stochastic_policy.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "######### VANILLA POLICY GRADIENT ###########\n",
    "#############################################\n",
    "\n",
    "def vanilla_policy_gradient():\n",
    "    for i in range(epochs):\n",
    "        batch_observations = [] \n",
    "        batch_actions = []      \n",
    "        batch_weights = []      \n",
    "        batch_returns = []      \n",
    "        batch_lengths = []      \n",
    "\n",
    "        observation = env.reset()\n",
    "        # print(\"observation actual dimension: \", observation.shape)\n",
    "        # print(\"observation: \", observation)\n",
    "        done = False            \n",
    "        rewards_in_episode = []            # list for rewards in the current episode\n",
    "\n",
    "        # First step: collect experience by simulating the environment with current policy\n",
    "        while True:\n",
    "            # print(\"vroum\")\n",
    "            batch_observations.append(observation.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            action = choose_action(observation)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_actions.append(action)\n",
    "            rewards_in_episode.append(reward)\n",
    "\n",
    "            if done:\n",
    "                # print(\"\\ncrash\")\n",
    "                # if episode is over, record info about episode\n",
    "                episode_return, episode_length = sum(rewards_in_episode), len(rewards_in_episode)\n",
    "                batch_returns.append(episode_return)\n",
    "                batch_lengths.append(episode_length)\n",
    "\n",
    "                # the weight for each logprobability(action|observation)\n",
    "                batch_weights += [episode_return] * episode_length\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                observation, done, rewards_in_episode = env.reset(), False, []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_observations) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # Step second: update the policy\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(torch.as_tensor(batch_observations, dtype=torch.float32).to(dev),\n",
    "                                  torch.as_tensor(batch_actions, dtype=torch.int32).to(dev),\n",
    "                                  torch.as_tensor(batch_weights, dtype=torch.float32).to(dev)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t episode_length: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_returns), np.mean(batch_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 21.628 \t return: 9.036 \t episode_length: 12.383\n",
      "epoch:   1 \t loss: 22.697 \t return: 10.112 \t episode_length: 13.877\n",
      "epoch:   2 \t loss: 25.223 \t return: 11.685 \t episode_length: 16.190\n",
      "epoch:   3 \t loss: 26.539 \t return: 14.105 \t episode_length: 19.667\n",
      "epoch:   4 \t loss: 25.200 \t return: 14.839 \t episode_length: 21.000\n",
      "epoch:   5 \t loss: 25.079 \t return: 18.807 \t episode_length: 26.447\n",
      "epoch:   6 \t loss: 22.997 \t return: 17.824 \t episode_length: 25.225\n",
      "epoch:   7 \t loss: 20.117 \t return: 19.799 \t episode_length: 28.194\n",
      "epoch:   8 \t loss: 17.901 \t return: 19.466 \t episode_length: 27.595\n",
      "epoch:   9 \t loss: 17.739 \t return: 21.223 \t episode_length: 29.971\n",
      "epoch:  10 \t loss: 14.885 \t return: 19.414 \t episode_length: 27.757\n",
      "epoch:  11 \t loss: 15.296 \t return: 19.977 \t episode_length: 28.333\n",
      "epoch:  12 \t loss: 13.586 \t return: 20.407 \t episode_length: 28.714\n",
      "epoch:  13 \t loss: 12.723 \t return: 20.867 \t episode_length: 29.618\n",
      "epoch:  14 \t loss: 11.307 \t return: 20.520 \t episode_length: 29.029\n",
      "epoch:  15 \t loss: 9.694 \t return: 20.603 \t episode_length: 29.343\n",
      "epoch:  16 \t loss: 7.817 \t return: 20.619 \t episode_length: 29.286\n",
      "epoch:  17 \t loss: 7.089 \t return: 20.532 \t episode_length: 29.257\n",
      "epoch:  18 \t loss: 6.196 \t return: 20.972 \t episode_length: 29.853\n",
      "epoch:  19 \t loss: 4.092 \t return: 20.921 \t episode_length: 29.794\n",
      "epoch:  20 \t loss: 4.385 \t return: 20.872 \t episode_length: 29.529\n",
      "epoch:  21 \t loss: 4.275 \t return: 20.700 \t episode_length: 29.286\n",
      "epoch:  22 \t loss: 2.858 \t return: 21.044 \t episode_length: 30.000\n",
      "epoch:  23 \t loss: 3.375 \t return: 21.161 \t episode_length: 30.000\n",
      "epoch:  24 \t loss: 2.883 \t return: 20.475 \t episode_length: 29.257\n",
      "epoch:  25 \t loss: 2.202 \t return: 20.835 \t episode_length: 29.618\n",
      "epoch:  26 \t loss: 3.511 \t return: 20.445 \t episode_length: 28.771\n",
      "epoch:  27 \t loss: 2.074 \t return: 20.233 \t episode_length: 28.857\n",
      "epoch:  28 \t loss: 1.511 \t return: 20.196 \t episode_length: 28.800\n",
      "epoch:  29 \t loss: 2.641 \t return: 20.925 \t episode_length: 30.000\n",
      "epoch:  30 \t loss: 2.452 \t return: 20.446 \t episode_length: 29.086\n",
      "epoch:  31 \t loss: 2.567 \t return: 21.092 \t episode_length: 30.000\n",
      "epoch:  32 \t loss: 1.020 \t return: 21.197 \t episode_length: 30.000\n",
      "epoch:  33 \t loss: 1.148 \t return: 20.987 \t episode_length: 30.000\n",
      "epoch:  34 \t loss: 2.347 \t return: 20.888 \t episode_length: 29.735\n",
      "epoch:  35 \t loss: 1.734 \t return: 20.608 \t episode_length: 29.559\n",
      "epoch:  36 \t loss: 1.567 \t return: 20.116 \t episode_length: 28.686\n",
      "epoch:  37 \t loss: 1.996 \t return: 21.070 \t episode_length: 29.824\n",
      "epoch:  38 \t loss: 1.357 \t return: 20.966 \t episode_length: 30.000\n",
      "epoch:  39 \t loss: 1.680 \t return: 20.434 \t episode_length: 29.314\n",
      "epoch:  40 \t loss: 1.768 \t return: 20.452 \t episode_length: 29.441\n",
      "epoch:  41 \t loss: 1.688 \t return: 20.934 \t episode_length: 30.000\n",
      "epoch:  42 \t loss: 1.825 \t return: 20.131 \t episode_length: 28.971\n",
      "epoch:  43 \t loss: 1.621 \t return: 21.019 \t episode_length: 30.000\n",
      "epoch:  44 \t loss: 1.050 \t return: 20.472 \t episode_length: 29.229\n",
      "epoch:  45 \t loss: 2.045 \t return: 21.234 \t episode_length: 30.000\n",
      "epoch:  46 \t loss: 1.983 \t return: 20.922 \t episode_length: 30.000\n",
      "epoch:  47 \t loss: 1.420 \t return: 20.803 \t episode_length: 29.706\n",
      "epoch:  48 \t loss: 2.098 \t return: 20.855 \t episode_length: 29.400\n",
      "epoch:  49 \t loss: 1.541 \t return: 20.708 \t episode_length: 29.588\n",
      "epoch:  50 \t loss: 2.220 \t return: 20.894 \t episode_length: 29.941\n",
      "epoch:  51 \t loss: 1.976 \t return: 20.250 \t episode_length: 29.086\n",
      "epoch:  52 \t loss: 1.640 \t return: 21.083 \t episode_length: 30.000\n",
      "epoch:  53 \t loss: 2.478 \t return: 20.677 \t episode_length: 29.471\n",
      "epoch:  54 \t loss: 2.330 \t return: 20.615 \t episode_length: 29.588\n",
      "epoch:  55 \t loss: 2.592 \t return: 20.820 \t episode_length: 30.000\n",
      "epoch:  56 \t loss: 2.362 \t return: 20.495 \t episode_length: 29.286\n",
      "epoch:  57 \t loss: 2.476 \t return: 20.413 \t episode_length: 29.286\n",
      "epoch:  58 \t loss: 3.085 \t return: 21.045 \t episode_length: 29.882\n",
      "epoch:  59 \t loss: 3.131 \t return: 20.900 \t episode_length: 29.676\n",
      "epoch:  60 \t loss: 2.882 \t return: 20.293 \t episode_length: 29.114\n",
      "epoch:  61 \t loss: 3.396 \t return: 21.032 \t episode_length: 30.000\n",
      "epoch:  62 \t loss: 3.955 \t return: 20.539 \t episode_length: 29.429\n",
      "epoch:  63 \t loss: 3.682 \t return: 20.814 \t episode_length: 29.676\n",
      "epoch:  64 \t loss: 3.400 \t return: 20.387 \t episode_length: 29.257\n",
      "epoch:  65 \t loss: 3.433 \t return: 20.969 \t episode_length: 30.000\n",
      "epoch:  66 \t loss: 3.011 \t return: 20.425 \t episode_length: 29.114\n",
      "epoch:  67 \t loss: 3.542 \t return: 20.792 \t episode_length: 29.794\n",
      "epoch:  68 \t loss: 3.458 \t return: 20.049 \t episode_length: 28.714\n",
      "epoch:  69 \t loss: 3.250 \t return: 19.764 \t episode_length: 28.500\n",
      "epoch:  70 \t loss: 2.805 \t return: 20.596 \t episode_length: 29.429\n",
      "epoch:  71 \t loss: 2.970 \t return: 21.044 \t episode_length: 30.000\n",
      "epoch:  72 \t loss: 3.053 \t return: 20.187 \t episode_length: 29.114\n",
      "epoch:  73 \t loss: 2.491 \t return: 20.999 \t episode_length: 30.000\n",
      "epoch:  74 \t loss: 3.845 \t return: 20.908 \t episode_length: 29.618\n",
      "epoch:  75 \t loss: 3.881 \t return: 21.238 \t episode_length: 30.000\n",
      "epoch:  76 \t loss: 3.896 \t return: 20.932 \t episode_length: 30.000\n",
      "epoch:  77 \t loss: 4.026 \t return: 20.793 \t episode_length: 29.706\n",
      "epoch:  78 \t loss: 5.510 \t return: 20.789 \t episode_length: 30.000\n",
      "epoch:  79 \t loss: 5.325 \t return: 20.875 \t episode_length: 29.618\n",
      "epoch:  80 \t loss: 6.879 \t return: 21.000 \t episode_length: 30.000\n",
      "epoch:  81 \t loss: 7.779 \t return: 20.439 \t episode_length: 29.257\n",
      "epoch:  82 \t loss: 8.413 \t return: 20.945 \t episode_length: 30.000\n",
      "epoch:  83 \t loss: 9.451 \t return: 20.065 \t episode_length: 28.800\n",
      "epoch:  84 \t loss: 10.214 \t return: 20.885 \t episode_length: 29.618\n",
      "epoch:  85 \t loss: 10.464 \t return: 21.277 \t episode_length: 30.000\n",
      "epoch:  86 \t loss: 10.479 \t return: 21.087 \t episode_length: 30.000\n",
      "epoch:  87 \t loss: 10.575 \t return: 20.531 \t episode_length: 29.343\n",
      "epoch:  88 \t loss: 11.351 \t return: 20.852 \t episode_length: 29.618\n",
      "epoch:  89 \t loss: 11.434 \t return: 19.267 \t episode_length: 27.486\n",
      "epoch:  90 \t loss: 12.980 \t return: 21.202 \t episode_length: 29.824\n",
      "epoch:  91 \t loss: 13.263 \t return: 19.406 \t episode_length: 27.459\n",
      "epoch:  92 \t loss: 14.232 \t return: 21.256 \t episode_length: 30.000\n",
      "epoch:  93 \t loss: 14.031 \t return: 20.991 \t episode_length: 29.882\n",
      "epoch:  94 \t loss: 14.685 \t return: 20.514 \t episode_length: 29.000\n",
      "epoch:  95 \t loss: 14.454 \t return: 20.591 \t episode_length: 29.400\n",
      "epoch:  96 \t loss: 14.579 \t return: 19.448 \t episode_length: 27.459\n",
      "epoch:  97 \t loss: 15.572 \t return: 20.259 \t episode_length: 28.600\n",
      "epoch:  98 \t loss: 14.371 \t return: 20.219 \t episode_length: 28.686\n",
      "epoch:  99 \t loss: 13.880 \t return: 20.322 \t episode_length: 28.743\n",
      "epoch: 100 \t loss: 13.907 \t return: 19.993 \t episode_length: 28.250\n",
      "epoch: 101 \t loss: 13.567 \t return: 19.341 \t episode_length: 27.514\n",
      "epoch: 102 \t loss: 14.345 \t return: 20.677 \t episode_length: 28.886\n",
      "epoch: 103 \t loss: 13.264 \t return: 18.829 \t episode_length: 27.189\n",
      "epoch: 104 \t loss: 13.600 \t return: 18.783 \t episode_length: 26.684\n",
      "epoch: 105 \t loss: 13.521 \t return: 20.374 \t episode_length: 28.611\n",
      "epoch: 106 \t loss: 12.655 \t return: 20.106 \t episode_length: 28.444\n",
      "epoch: 107 \t loss: 11.292 \t return: 19.753 \t episode_length: 27.351\n",
      "epoch: 108 \t loss: 9.523 \t return: 18.396 \t episode_length: 25.949\n",
      "epoch: 109 \t loss: 8.767 \t return: 18.247 \t episode_length: 25.024\n",
      "epoch: 110 \t loss: 7.130 \t return: 16.328 \t episode_length: 22.467\n",
      "epoch: 111 \t loss: 5.813 \t return: 17.054 \t episode_length: 23.279\n",
      "epoch: 112 \t loss: 5.755 \t return: 16.941 \t episode_length: 23.068\n",
      "epoch: 113 \t loss: 4.666 \t return: 14.494 \t episode_length: 19.647\n",
      "epoch: 114 \t loss: 4.293 \t return: 14.663 \t episode_length: 19.731\n",
      "epoch: 115 \t loss: 4.410 \t return: 15.772 \t episode_length: 21.271\n",
      "epoch: 116 \t loss: 3.161 \t return: 13.988 \t episode_length: 18.611\n",
      "epoch: 117 \t loss: 4.433 \t return: 15.500 \t episode_length: 20.896\n",
      "epoch: 118 \t loss: 4.163 \t return: 16.011 \t episode_length: 21.383\n",
      "epoch: 119 \t loss: 4.349 \t return: 15.743 \t episode_length: 21.681\n",
      "epoch: 120 \t loss: 5.241 \t return: 14.914 \t episode_length: 20.078\n",
      "epoch: 121 \t loss: 5.957 \t return: 17.625 \t episode_length: 24.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 122 \t loss: 6.761 \t return: 17.312 \t episode_length: 23.953\n",
      "epoch: 123 \t loss: 9.135 \t return: 19.414 \t episode_length: 26.789\n",
      "epoch: 124 \t loss: 9.369 \t return: 17.911 \t episode_length: 25.049\n",
      "epoch: 125 \t loss: 11.189 \t return: 19.565 \t episode_length: 27.026\n",
      "epoch: 126 \t loss: 11.726 \t return: 18.392 \t episode_length: 25.744\n",
      "epoch: 127 \t loss: 12.647 \t return: 20.552 \t episode_length: 28.743\n",
      "epoch: 128 \t loss: 13.569 \t return: 19.910 \t episode_length: 28.083\n",
      "epoch: 129 \t loss: 13.663 \t return: 21.163 \t episode_length: 30.000\n",
      "epoch: 130 \t loss: 14.135 \t return: 21.065 \t episode_length: 29.912\n",
      "epoch: 131 \t loss: 14.665 \t return: 20.465 \t episode_length: 28.914\n",
      "epoch: 132 \t loss: 14.565 \t return: 20.578 \t episode_length: 29.257\n",
      "epoch: 133 \t loss: 14.738 \t return: 20.024 \t episode_length: 28.167\n",
      "epoch: 134 \t loss: 15.066 \t return: 20.639 \t episode_length: 29.200\n",
      "epoch: 135 \t loss: 14.081 \t return: 20.580 \t episode_length: 29.257\n",
      "epoch: 136 \t loss: 13.544 \t return: 20.856 \t episode_length: 29.429\n",
      "epoch: 137 \t loss: 11.737 \t return: 19.874 \t episode_length: 28.600\n",
      "epoch: 138 \t loss: 9.889 \t return: 20.819 \t episode_length: 29.971\n",
      "epoch: 139 \t loss: 9.346 \t return: 20.786 \t episode_length: 29.500\n",
      "epoch: 140 \t loss: 6.417 \t return: 20.922 \t episode_length: 30.000\n",
      "epoch: 141 \t loss: 5.483 \t return: 20.630 \t episode_length: 29.400\n",
      "epoch: 142 \t loss: 5.260 \t return: 21.195 \t episode_length: 30.000\n",
      "epoch: 143 \t loss: 3.996 \t return: 20.906 \t episode_length: 29.912\n",
      "epoch: 144 \t loss: 3.017 \t return: 20.563 \t episode_length: 29.400\n",
      "epoch: 145 \t loss: 3.153 \t return: 20.567 \t episode_length: 29.286\n",
      "epoch: 146 \t loss: 2.045 \t return: 20.991 \t episode_length: 30.000\n",
      "epoch: 147 \t loss: 1.947 \t return: 20.314 \t episode_length: 29.257\n",
      "epoch: 148 \t loss: 1.712 \t return: 20.477 \t episode_length: 29.257\n",
      "epoch: 149 \t loss: 1.416 \t return: 20.428 \t episode_length: 29.400\n",
      "epoch: 150 \t loss: 1.043 \t return: 20.368 \t episode_length: 29.286\n",
      "epoch: 151 \t loss: 1.267 \t return: 20.991 \t episode_length: 30.000\n",
      "epoch: 152 \t loss: 1.187 \t return: 20.363 \t episode_length: 29.000\n",
      "epoch: 153 \t loss: 1.076 \t return: 20.814 \t episode_length: 30.000\n",
      "epoch: 154 \t loss: 1.579 \t return: 20.995 \t episode_length: 30.000\n",
      "epoch: 155 \t loss: 0.889 \t return: 20.932 \t episode_length: 30.000\n",
      "epoch: 156 \t loss: 0.866 \t return: 20.357 \t episode_length: 29.286\n",
      "epoch: 157 \t loss: 1.519 \t return: 20.873 \t episode_length: 29.429\n",
      "epoch: 158 \t loss: 0.918 \t return: 20.214 \t episode_length: 28.771\n",
      "epoch: 159 \t loss: 0.659 \t return: 21.020 \t episode_length: 30.000\n",
      "epoch: 160 \t loss: 1.195 \t return: 20.628 \t episode_length: 29.676\n",
      "epoch: 161 \t loss: 0.446 \t return: 21.099 \t episode_length: 29.971\n",
      "epoch: 162 \t loss: 0.877 \t return: 21.152 \t episode_length: 30.000\n",
      "epoch: 163 \t loss: 0.853 \t return: 20.368 \t episode_length: 29.343\n",
      "epoch: 164 \t loss: 0.769 \t return: 20.873 \t episode_length: 29.853\n",
      "epoch: 165 \t loss: 0.672 \t return: 20.961 \t episode_length: 30.000\n",
      "epoch: 166 \t loss: 1.373 \t return: 20.506 \t episode_length: 29.471\n",
      "epoch: 167 \t loss: 0.652 \t return: 19.864 \t episode_length: 28.583\n",
      "epoch: 168 \t loss: 1.070 \t return: 20.784 \t episode_length: 29.429\n",
      "epoch: 169 \t loss: 0.862 \t return: 20.877 \t episode_length: 30.000\n",
      "epoch: 170 \t loss: 1.067 \t return: 20.514 \t episode_length: 29.441\n",
      "epoch: 171 \t loss: 0.875 \t return: 20.961 \t episode_length: 30.000\n",
      "epoch: 172 \t loss: 0.769 \t return: 21.032 \t episode_length: 29.882\n",
      "epoch: 173 \t loss: 0.778 \t return: 21.373 \t episode_length: 30.000\n",
      "epoch: 174 \t loss: 0.984 \t return: 20.885 \t episode_length: 29.882\n",
      "epoch: 175 \t loss: 0.870 \t return: 21.197 \t episode_length: 30.000\n",
      "epoch: 176 \t loss: 0.457 \t return: 20.903 \t episode_length: 30.000\n",
      "epoch: 177 \t loss: 1.080 \t return: 20.730 \t episode_length: 29.676\n",
      "epoch: 178 \t loss: 0.867 \t return: 20.281 \t episode_length: 28.771\n",
      "epoch: 179 \t loss: 0.574 \t return: 21.344 \t episode_length: 30.000\n",
      "epoch: 180 \t loss: 1.390 \t return: 19.959 \t episode_length: 28.361\n",
      "epoch: 181 \t loss: 1.197 \t return: 20.555 \t episode_length: 29.143\n",
      "epoch: 182 \t loss: 0.874 \t return: 21.020 \t episode_length: 30.000\n",
      "epoch: 183 \t loss: 1.309 \t return: 20.579 \t episode_length: 29.500\n",
      "epoch: 184 \t loss: 0.867 \t return: 20.932 \t episode_length: 30.000\n",
      "epoch: 185 \t loss: 1.079 \t return: 20.495 \t episode_length: 29.257\n",
      "epoch: 186 \t loss: 1.153 \t return: 20.873 \t episode_length: 30.000\n",
      "epoch: 187 \t loss: 0.966 \t return: 20.464 \t episode_length: 29.314\n",
      "epoch: 188 \t loss: 1.476 \t return: 20.883 \t episode_length: 29.647\n",
      "epoch: 189 \t loss: 1.298 \t return: 20.961 \t episode_length: 29.765\n",
      "epoch: 190 \t loss: 1.088 \t return: 21.108 \t episode_length: 30.000\n",
      "epoch: 191 \t loss: 1.145 \t return: 20.058 \t episode_length: 28.857\n",
      "epoch: 192 \t loss: 1.726 \t return: 20.865 \t episode_length: 29.735\n",
      "epoch: 193 \t loss: 1.525 \t return: 20.524 \t episode_length: 29.765\n",
      "epoch: 194 \t loss: 2.028 \t return: 20.411 \t episode_length: 29.057\n",
      "epoch: 195 \t loss: 1.336 \t return: 21.079 \t episode_length: 30.000\n",
      "epoch: 196 \t loss: 1.714 \t return: 20.587 \t episode_length: 29.588\n",
      "epoch: 197 \t loss: 1.762 \t return: 20.405 \t episode_length: 29.229\n",
      "epoch: 198 \t loss: 2.160 \t return: 20.386 \t episode_length: 28.800\n",
      "epoch: 199 \t loss: 2.497 \t return: 20.756 \t episode_length: 30.000\n"
     ]
    }
   ],
   "source": [
    "vanilla_policy_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DECOMMENT TO SAVE THE MODEL, BE CAREFUL OF ERASING ANOTHER ONE ###\n",
    "# torch.save(net_stochastic_policy.state_dict(), \"models/model_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Average score of the policy:  70.37688783112466\n"
     ]
    }
   ],
   "source": [
    "###### EVALUATION ############\n",
    "\n",
    "def run_episode(env, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = choose_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    print(\"episode done\")\n",
    "    return total_reward\n",
    "\n",
    "policy_scores = [run_episode(env) for _ in range(20)] #100\n",
    "print(\"Average score of the policy: \", np.mean(policy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "episode done\n",
      "Press r to restart simulation, otherwise another key\n"
     ]
    }
   ],
   "source": [
    "###### DEMONSTRATION ############\n",
    "\n",
    "go = True\n",
    "while go:\n",
    "    for _ in range(5):\n",
    "        run_episode(env, True)\n",
    "    print(\"Press r to restart simulation, otherwise another key\")\n",
    "    if read_key() != \"r\":\n",
    "        go = False\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
